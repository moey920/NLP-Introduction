## 나이브 베이즈

> 나이브 베이즈 기반 감정 분석은 주어진 텍스트가 특정 감정을 나타낼 확률을 예측하는 문제로 정의

**나이브 베이즈의 원리**

`[텍스트 1] : 영상미가 | 뛰어나고 | 너무너무 | 재미있었어요`
-> P(감정|텍스트) = ?

- 조건부 확률을 사용한다!
    - 베이즈 정리를 사용하여 텍스트의 감정 발생 확률을 추정
    - `P(감정|텍스트) = (P(텍스트|감정) × P(감정)) / P(텍스트)`

> 감정의 발생 확률과 텍스트를 구성하는 단어들의 가능도(likelihood)로 텍스트의 감정을 예측

`[텍스트 1] : 영상미가 | 뛰어나고 | 너무너무 | 재미있었어요`
->
`[텍스트 1의 감정] : 해당 감정 내 단어들이 발생할 가능성 × 감정의 발생 확률`

### 단어의 가능도

`P(단어|감정) = (감정 내 단어의 빈도수) / (감정 내 모든 단어의 빈도수)`  
`P("재미있었어요"|기쁨) = (기쁨을 표현하는 문서 내 "재미있었어요"의 빈도수) / (기쁨을 표현하는 문서 내 모든 단어의 빈도수)`  

> 텍스트 데이터에서는 가능도는 단어의 빈도수로 추정

### 감정의 발생 확률

`P(감정) = (해당 감정을 표현하는 문서의 수) / (데이터 내 모든 문서의 수)`  
`P(기쁨) = (기쁨을 표현하는 리뷰의 수) / (전체 리뷰의 수)`  

> 감정의 발생 확률은 주어진 텍스트 데이터 내 해당 감정을 표현하는 문서의 비율로 추정

- 텍스트의 감정
    - 텍스트의 감정별 확률값 중 **최대 확률값**을 나타내는 감정을 해당 문서의 감정으로 예측

### 나이브 베이즈 학습

나이브 베이즈 기법에서는 각 감정 내 단어의 가능도(likelihood) 를 기반으로 문장의 감정을 예측합니다.

```
import pandas as pd

# 텍스트 데이터와 특정 감정을 입력 받으며, 해당 감정을 나타내는 문서를 filtered_texts에 저장합니다
def cal_partial_freq(texts, emotion):
    partial_freq = dict()
    filtered_texts = texts[texts['emotion']==emotion]
    filtered_texts = filtered_texts['sentence']
    
    # 전체 데이터 내 각 단어별 빈도수를 입력해 주는 부분을 구현하세요.
    for sentence in filtered_texts :
        words = sentence.rstrip().split()
        for word in words :
            if word not in partial_freq :
                partial_freq[word] = 1
            else :
                partial_freq[word] += 1
                
    return partial_freq

# 특정 감정별 문서 내 전체 단어의 빈도 수를 계산하여 반환
def cal_total_freq(partial_freq):
    total = 0
    # partial_freq 딕셔너리에서 감정별로 문서 내 전체 단어의 빈도 수를 계산하여 반환하는 부분을 구현하세요.
    for word, freq in partial_freq.items() :
        total += freq
    
    return total

# Emotions dataset for NLP를 불러옵니다.
data = pd.read_csv("emotions_train.txt", delimiter=';', header=None, names=['sentence','emotion'])

# happy가 joy라는 감정을 표현하는 문장에서 발생할 가능도를 구하세요.
joy_counter = cal_partial_freq(data, "joy")
joy_likelihood = joy_counter["happy"] / cal_total_freq(joy_counter)
print(joy_likelihood)

# happy가 sadness라는 감정을 표현하는 문장에서 발생할 가능도를 구하세요.
sad_counter = cal_partial_freq(data, "sadness")
sad_likelihood = sad_counter["happy"] / cal_total_freq(sad_counter)
print(sad_likelihood)

# can이 surprise라는 감정을 표현하는 문장에서 발생할 가능도를 구하세요.
sup_counter = cal_partial_freq(data, "surprise")
sup_likelihood = sup_counter["can"] / cal_total_freq(sup_counter)
print(sup_likelihood)
```

# 나이브 베이즈 기반 감정 예측

## 스무딩 (smoothing)

- 학습 데이터 내 재미있었어요의 빈도 = 0
    - 학습 데이터 내 존재하지 않은 단어가 포함된 문장의 감정 발생 확률은 0

`P("재미있었어요"|기쁨) = (기쁨을 표현하는 문서 내 "재미있었어요"의 빈도수) + 1 / (기쁨을 표현하는 문서 내 모든 단어의 빈도수) + 1`

> 스무딩(smoothing)을 통해 학습 데이터 내 존재하지 않은 단어의 빈도수를 보정

## 소수

```
[텍스트 1] : 영상미가 | 뛰어나고 | 너무너무 | 재미있었어요
[텍스트 1이 기쁨을 나타낼 확률] : 0.52 × ⋯ × 0.75 × 0.22
[텍스트 1이 분노를 나타낼 확률] : 0.1 × ⋯ × 0.001 × 0.35
```

- 단어의 감정별 가능도와 감정의 발생 확률은 모두 **소수로 표현**
- 연속적으로 소수를 곱하면 결괏값은 끊임없이 감소
- 감정예측을 하려는 문장에서 단어의 수가 많아질수록 텍스트의 확률값은 컴퓨터가 **처리할 수 있는 소수점의 범위보다 작아질 수 있음**
    ```
    [텍스트 1] : 동해물과 | 백두산이 | 마르고 | 닳도록 …
    [텍스트 1이 기쁨을 나타낼 확률] : 0.52 × ⋯ × 0.12 × 0.12 = 0.0000000000000000000000037291
    ```
- 

## 로그

로그를 사용하면 끊임없이 숫자가 작아지는 것을 방지

```
log10 0.1 × 0.1 = log10 0.1 + log10 (0.1) = −2
log10 0.1 × 0.1 × 0.1 = log10 0.1 + log10 0.1 + log10 0.1 = −3
log10 0.1 × 0.1 × 0.1 × 0.1 = log10 0.1 + log10 0.1 + log10 0.1 + log10 0.1 = −4
```
```
[텍스트 1] : 영상미가 | 뛰어나고 | 너무너무 | 재미있었어요
[텍스트 1이 기쁨을 나타낼 확률] : log(P("영상미가"|기쁨)) + ⋯ + log(P("재미있었어요"|기쁨)) + log(P(기쁨))
```

> 로그 확률값의 합으로 텍스트의 감정을 예측

### 나이브 베이즈 기반 감정 예측 실습(2)

```
import pandas as pd
import numpy as np

def cal_partial_freq(texts, emotion):
    filtered_texts = texts[texts['emotion'] == emotion]
    filtered_texts = filtered_texts['sentence']
    partial_freq = dict()

    for sent in filtered_texts :
        words = sent.rstrip().split()
        for word in words :
            if word not in partial_freq :
                partial_freq[word] = 1
            else :
                partial_freq[word] += 1

    return partial_freq

def cal_total_freq(partial_freq):
    total = 0
    # partial_freq 딕셔너리에서 감정별로 문서 내 전체 단어의 빈도 수를 계산하여 반환하는 부분
    for word, freq in partial_freq.items() :
        total += freq
    
    return total

def cal_prior_prob(data, emotion):
    # 전체 데이터 중에서 특정 감정을 가지는 데이터를 filtered_texts로 저장
    filtered_texts = data[data['emotion'] == emotion]
    # data 내 특정 감정의 로그발생 확률을 반환하는 부분을 구현하세요.

    return np.log(len(filtered_texts)/len(data))

def predict_emotion(sent, data):
    emotions = ['anger', 'love', 'sadness', 'fear', 'joy', 'surprise']
    predictions = []
    train_txt = pd.read_csv(data, delimiter=';', header=None, names=['sentence', 'emotion'])

    # sent의 각 감정별 로그 확률을 predictions 리스트에 저장하세요.
    for emotion in emotions: # 각 감정
        prob = 0 # 각 감정 별 로그의 합을 저장
        for word in sent.split() : # 문장을 구성하는 단어의 리스트
            emotion_counter = cal_partial_freq(train_txt, emotion)
            prob += np.log((emotion_counter[word] + 10) / (cal_total_freq(emotion_counter) + 10)) # 각 단어의 로그 가능도
        prob += cal_prior_prob(train_txt, emotion) # 해당 감정
        predictions.append((emotion, prob))
    predictions.sort(key = lambda a : a[1])

    return predictions[-1]

# 아래 문장의 예측된 감정을 확인해보세요.
test_sent = "i really want to go and enjoy this party"
predicted = predict_emotion(test_sent, "emotions_train.txt")
print(predicted)
```

# scikit-learn을 통한 나이브 베이즈 구현

> **scikit-learn**은 각종 데이터 전처리 및 머신 러닝 모델을 간편한 형태로 제공하는 파이썬 라이브러리

Example
```
from sklearn.feature_extraction.text import CountVectorizer

doc = ["i am very happy", "this product is really great"]
emotion = ["happy", "excited"]

cv = CountVectorizer() # CountVectorizer 클래스의 cv 객체 생성
csr_doc_matrix = cv.fit_transform(doc) # 수치형 데이터로 변환하는 방법

# 각 단어 및 문장별 고유 ID 부여 및 단어의 빈도수를 계산
print(csr_doc_matrix) # (0, 0) 1, (0, 7) 1

from sklearn.naive_bayes import MultinomialNB

clf = MultinomialNB()
# CountVectorizer로 변환된 텍스트 데이터를 사용
clf.fit(csr_doc_matrix, emotion) # 인자로 학습 데이터(벡터)와 감정(레이블)을 넣어준다.

test_doc = ["i am really great"]

# 학습된 CountVectorizer 형태로 변환
transformed_test = cv.transform(test_doc)
pred = clf.predict(doc_vector)

print(pred) # array(['excited'], dtype='<U7')
```

### scikit-learn을 통한 나이브 베이즈 감정 분석 실습

```
import pandas as pd
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer

raw_text = pd.read_csv("emotions_train.txt", delimiter=';', header=None, names=['sentence','emotion'])
train_data = raw_text['sentence']
train_emotion = raw_text['emotion']

# CountVectorizer 객체인 변수 cv를 만들고, fit_transform 메소드로 train_data를 변환하세요.
cv = CountVectorizer()
transformed_text = cv.fit_transform(train_data)

# MultinomialNB 객체인 변수 clf를 만들고, fit 메소드로 2번에서 변환된 train_data와 train_emotion을 학습하세요.
clf = MultinomialNB()
clf.fit(transformed_text, train_emotion)

# 아래 문장의 감정을 예측하세요.
test_data = ['i am curious', 'i feel gloomy and tired', 'i feel more creative', 'i feel a little mellow today']
test_transformed_text = cv.transform(test_data)

test_result = clf.predict(test_transformed_text)
print(test_result) # ['surprise' 'sadness' 'joy' 'joy']
```

### 나이브 베이즈 기반 감정 분석 서비스(웹 서버)

``` 
nb_flask.py
# 경고문을 무시합니다.
import warnings
warnings.filterwarnings(action='ignore')

from flask import Flask, request, jsonify
import pickle

app = Flask(__name__)

@app.route('/predict', methods=['POST'])
def predict():
    json_ = request.json
    query = json_['infer_texts'] # 웹 서버로 전달된 문자열 리스트
    
    # 학습된 객체 cv 변수와 clf 변수를 이용해 전달된 문자열의 감정을 예측하는 코드를 작성하세요.
    predictions = clf.predict(cv.transform(query))

    # 예측된 결과를 response 딕셔너리에 "문서의 순서: 예측된 감점" 형태로 저장하세요.
    response = dict()
    for idx, pred in enumerate(predictions) : # enumerate() : 반복하는 객체 앞에 인덱스 값을 자동으로 생성해준다.
        response[idx] = pred
    
    return jsonify(response)

if __name__ == '__main__':
    with open('nb_model.pkl', 'rb') as f: # 모델 로드(cv, clf를 불러온다)
        cv, clf = pickle.load(f)
    
    app.run(host='0.0.0.0', port=8080)
```

``` 
main.py
import requests

# 아래 문장의 감정을 예측합니다.
test_data = ['i am happy', 'i want to go', 'i wake too early so i feel grumpy', 'i feel alarmed']

myobj = {"infer_texts": test_data}

x = requests.post("http://0.0.0.0:8080/predict", json = myobj)
print("감정 분석 결과: " + x.text)
```

# 기타 감정 분석 방법

> 감정 분석은 지도 학습(supervised learning) 기반의 분류 및 예측의 문제

- 감정 분석 + 머신러닝 : 학습 데이터에 감정(레이블)만 존재하면 머신러닝 알고리즘 학습이 가능
- **임베딩 벡터**를 사용하여, 머신러닝 알고리즘 적용이 가능 : 자연어 처리보다는 머신러닝에 더 근접하다.
    - 가장 간단한 방법으로 **단어 임베딩 벡터의 평균**을 사용
    - **단어 임베딩 벡터에 필터**를 적용하여 **CNN 기반**으로 감정 분류

## 예시: CNN

- 문자 임베딩 벡터에 필터를 적용하여 CNN 기반으로 감정 분류

```
[문장 임베딩 벡터]
영상미 : [0.12, 0.24, 0.913 …]   ===>
뛰어나다 : [0.87, 0.53, 0.03 …]  ===>  필터 ===> 기쁨
너무너무 : [0.41, 0.23, 0.09 …]  ===>
재미있다 : [0.71, 0.51, 0.23 …]  ===>

[단어 임베딩 벡터]
영 : [0.52, 0.24, 0.33 …] ===>
상 : [0.57, 0.13, 0.12 …] ===> 필터 ===> 기쁨
미 : [0.17, 0.83, 0.63 …] ===>
뛰 : [0.07, 0.93, 0.33 …] ===>
```

## 예시(2): RNN

- LSTM, GRU를 활용하여 RNN 기반으로 분류 및 예측
    - 각 셀에 임베딩 벡터를 넣어주고, RNN 구조로 만들어 출력층에서 감정을 예측한다
    - 문장, 단어 단위 모두 가능
