# 감정 분석 서비스

- 텍스트 데이터의 종류
    - 뉴스, 백과 사전 같은 텍스트는 **객관적인 정보**를 제공
    - 리뷰, 소설 같은 텍스트는 저자의 **주관적인 평가나 감정**을 표현

## 감정분석이란? 

> 감정 분석(Sentiment analysis)은 텍스트 내에 표현되는 감정 및 평가를 식별하는 자연어 처리의 한 분야

- 대량의 텍스트가 있는 경우, 일일이 데이터를 하나씩 살펴보고 판단하기 어려움
- 비슷한 감정을 표현하는 문서는 **유사한 단어 구성 및 언어적 특징**을 보일 것을 가정
- 텍스트 내 감정을 분류하거나 긍정/부정의 정도를 점수화(감정 : 분노 or 부정 : 0.75점..)
    - 분류 / 예측 모델을 만드는 서비스이다.
- 머신러닝 기반 감정 분석 서비스의 경우, 데이터를 통한 모델 학습부터 시작
    - 데이터에 레이블이 존재해야 한다.(**지도학습**)
    - 학습된 머신러닝 모델을 통해 신규 텍스트의 감정을 예측

### 모델 학습을 위한 데이터 분할

본격적으로 모델을 만들어 보기에 앞서 주어진 데이터를 학습 데이터와 평가 데이터로 나누는 방법에 대해서 배워보도록 하겠습니다.

학습 데이터란 감정 분석 모델을 훈련 시키기 위해 문장과 해당 문장의 감정이 포함되어 있는 데이터셋을 의미합니다.

평가 데이터란 학습된 모델의 성능을 평가하기 위해 학습에 포함되지 않은 데이터셋을 의미합니다.

이번 과정에서 `Emotions dataset for NLP` 데이터셋을 활용하여 **문장별 감정 분석**을 진행해 볼 예정입니다. 본 데이터셋의 각 줄은 아래와 같이 문장;감정의 형태로 구성이 되어 있습니다.

`i didnt feel humiliated;sadness`

```
from sklearn.model_selection import train_test_split

# 파일을 읽어오세요.
data = []
with open('emotions_train.txt', 'r') as f:
    for line in f :
        sentence, emotion = line.rstrip().split(';')
        data.append((sentence, emotion))
# print(data)

# 읽어온 파일을 학습 데이터와 평가 데이터로 분할하세요.
train, test = train_test_split(data, test_size = 0.2, random_state = 7)

# 학습 데이터셋의 문장과 감정을 분리하세요.
Xtrain = []
Ytrain = []

for train_data in train :
    Xtrain.append(train_data[0])
    Ytrain.append(train_data[1])

print(len(Xtrain))
print(set(Ytrain))

# 평가 데이터셋의 문장과 감정을 분리하세요.
Xtest = []
Ytest = []

for test_data in test :
    Xtest.append(test_data[0])
    Ytest.append(test_data[1])

print(len(Xtest))
print(set(Ytest))
```


## 나이브 베이즈

> 나이브 베이즈 기반 감정 분석은 주어진 텍스트가 특정 감정을 나타낼 확률을 예측하는 문제로 정의

**나이브 베이즈의 원리**

`[텍스트 1] : 영상미가 | 뛰어나고 | 너무너무 | 재미있었어요`
-> P(감정|텍스트) = ?

- 조건부 확률을 사용한다!
    - 베이즈 정리를 사용하여 텍스트의 감정 발생 확률을 추정
    - `P(감정|텍스트) = (P(텍스트|감정) × P(감정)) / P(텍스트)`

> 감정의 발생 확률과 텍스트를 구성하는 단어들의 가능도(likelihood)로 텍스트의 감정을 예측

`[텍스트 1] : 영상미가 | 뛰어나고 | 너무너무 | 재미있었어요`
->
`[텍스트 1의 감정] : 해당 감정 내 단어들이 발생할 가능성 × 감정의 발생 확률`

### 단어의 가능도

`P(단어|감정) = (감정 내 단어의 빈도수) / (감정 내 모든 단어의 빈도수)`
`P("재미있었어요"|기쁨) = (기쁨을 표현하는 문서 내 "재미있었어요"의 빈도수) / (기쁨을 표현하는 문서 내 모든 단어의 빈도수)`

> 텍스트 데이터에서는 가능도는 단어의 빈도수로 추정

### 감정의 발생 확률

`P(감정) = (해당 감정을 표현하는 문서의 수) / (데이터 내 모든 문서의 수)`
`P(기쁨) = (기쁨을 표현하는 리뷰의 수) / (전체 리뷰의 수)`

> 감정의 발생 확률은 주어진 텍스트 데이터 내 해당 감정을 표현하는 문서의 비율로 추정

- 텍스트의 감정
    - 텍스트의 감정별 확률값 중 **최대 확률값**을 나타내는 감정을 해당 문서의 감정으로 예측

### 나이브 베이즈 학습

나이브 베이즈 기법에서는 각 감정 내 단어의 가능도(likelihood) 를 기반으로 문장의 감정을 예측합니다.

```
import pandas as pd

# 텍스트 데이터와 특정 감정을 입력 받으며, 해당 감정을 나타내는 문서를 filtered_texts에 저장합니다
def cal_partial_freq(texts, emotion):
    partial_freq = dict()
    filtered_texts = texts[texts['emotion']==emotion]
    filtered_texts = filtered_texts['sentence']
    
    # 전체 데이터 내 각 단어별 빈도수를 입력해 주는 부분을 구현하세요.
    for sentence in filtered_texts :
        words = sentence.rstrip().split()
        for word in words :
            if word not in partial_freq :
                partial_freq[word] = 1
            else :
                partial_freq[word] += 1
                
    return partial_freq

# 특정 감정별 문서 내 전체 단어의 빈도 수를 계산하여 반환
def cal_total_freq(partial_freq):
    total = 0
    # partial_freq 딕셔너리에서 감정별로 문서 내 전체 단어의 빈도 수를 계산하여 반환하는 부분을 구현하세요.
    for word, freq in partial_freq.items() :
        total += freq
    
    return total

# Emotions dataset for NLP를 불러옵니다.
data = pd.read_csv("emotions_train.txt", delimiter=';', header=None, names=['sentence','emotion'])

# happy가 joy라는 감정을 표현하는 문장에서 발생할 가능도를 구하세요.
joy_counter = cal_partial_freq(data, "joy")
joy_likelihood = joy_counter["happy"] / cal_total_freq(joy_counter)
print(joy_likelihood)

# happy가 sadness라는 감정을 표현하는 문장에서 발생할 가능도를 구하세요.
sad_counter = cal_partial_freq(data, "sadness")
sad_likelihood = sad_counter["happy"] / cal_total_freq(sad_counter)
print(sad_likelihood)

# can이 surprise라는 감정을 표현하는 문장에서 발생할 가능도를 구하세요.
sup_counter = cal_partial_freq(data, "surprise")
sup_likelihood = sup_counter["can"] / cal_total_freq(sup_counter)
print(sup_likelihood)
```

## 나이브 베이즈 기반 감정 예측
03
학습 데이터 내 존재하지 않은 단어가 포함된 문장의 감정 발생 확률은 0
03
스무딩 (smoothing)
나이브 베이즈 기반 감정 예측
학습 데이터 내 재미있었어요의 빈도 = 0
P
෡
"재미있었어요" 기쁨) =
(기쁨을 표현하는 문서 내 "재미있었어요"의 빈도수)
(기쁨을 표현하는 문서 내 모든 단어의 빈도수)
= 0
스무딩(smoothing)을 통해 학습 데이터 내 존재하지 않은 단어의 빈도수를 보정
03
스무딩 (smoothing)
나이브 베이즈 기반 감정 예측
학습 데이터 내 재미있었어요의 빈도 = 0
P
෡
"재미있었어요" 기쁨) =
기쁨을 표현하는 문서 내 재미있었어요의 빈도수 + 1
기쁨을 표현하는 문서 내 모든 단어의 빈도수 + 1
단어의 감정별 가능도와 감정의 발생 확률은 모두 소수로 표현
03
소수
나이브 베이즈 기반 감정 예측
[텍스트 1] : 영상미가 | 뛰어나고 | 너무너무 | 재미있었어요
[텍스트 1이 기쁨을 나타낼 확률] : 0.52 × ⋯ × 0.75 × 0.22
[텍스트 1이 분노를 나타낼 확률] : 0.1 × ⋯ × 0.001 × 0.35
연속적으로 소수를 곱하면 결괏값은 끊임없이 감소
03
소수
나이브 베이즈 기반 감정 예측
0.1 × 0.1 = 0.01
0.1 × 0.1 × 0.1 = 0.001
0.1 × 0.1 × 0.1 × 0.1 = 0.0001
0.1 × 0.1 × 0.1 × 0.1 × 0.1 = 0.00001
.
.
.
단어의 수가 많아질수록 텍스트의 확률값은 컴퓨터가 처리할 수 있는
소수점의 범위보다 작아질 수 있음
03
소수
나이브 베이즈 기반 감정 예측
[텍스트 1] : 동해물과 | 백두산이 | 마르고 | 닳도록 …
[텍스트 1이 기쁨을 나타낼 확률] : 0.52 × ⋯ × 0.12 × 0.12 =
0.0000000000000000000000037291
로그를 사용하면 끊임없이 숫자가 작아지는 것을 방지
03
로그
나이브 베이즈 기반 감정 예측
log10 0.1 × 0.1 = log10 0.1 + log10 (0.1) = −2
log10 0.1 × 0.1 × 0.1 = log10 0.1 + log10 0.1 + log10 0.1 = −3
log10 0.1 × 0.1 × 0.1 × 0.1 = log10 0.1 + log10 0.1 + log10 0.1 + log10 0.1 = −4
.
.
.
로그 확률값의 합으로 텍스트의 감정을 예측
03
최종 나이브 베이즈
나이브 베이즈 기반 감정 예측
[텍스트 1] : 영상미가 | 뛰어나고 | 너무너무 | 재미있었어요
[텍스트 1이 기쁨을 나타낼 확률] : log(P
෡
"영상미가" 기쁨)) + ⋯ + log(෡
P "재미있었어요" 기쁨)) +
log(P(기쁨))
Confidential all rights reserved
scikit-learn을 통한 나이브 베이즈 구현
04
scikit-learn은 각종 데이터 전처리 및 머신 러닝 모델을
간편한 형태로 제공하는 파이썬 라이브러리
04
scikit-learn
scikit-learn을 통한 나이브 베이즈 구현
04
Example
from sklearn.feature_extraction.text import CountVectorizer
doc = ["i am very happy", "this product is really great"]
emotion = ["happy", "excited"]
cv = CountVectorizer()
csr_doc_matrix = cv.fit_transform(X_train)
# 각 단어 및 문장별 고유 ID 부여 및 단어의 빈도수를 계산
print(csr_doc_matrix) # (0, 0) 1, (0, 7) 1
scikit-learn
scikit-learn을 통한 나이브 베이즈 구현
04
Example
from sklearn.naive_bayes import MultinomialNB
doc = ["i am very happy", "this product is really great"]
emotion = ["happy", "excited"]
clf = MultinomialNB()
# CountVectorizer로 변환된 텍스트 데이터를 사용
clf.fit(csr_doc_matrix, emotion)
scikit-learn
scikit-learn을 통한 나이브 베이즈 구현
04
Example
from sklearn.naive_bayes import MultinomialNB
test_doc = ["i am really great"]
# 학습된 CountVectorizer 형태로 변환
transformed_test = cv.transform(test_doc)
pred = clf.predict(doc_vector)
print(pred) # array(['excited'], dtype='<U7')
scikit-learn
scikit-learn을 통한 나이브 베이즈 구현
Confidential all rights reserved
기타 감정 분석 방법
05
감정 분석은 지도 학습(supervised learning) 기반의 분류 및 예측의 문제
05
감정 분석
기타 감정 분석 방법
감정: 분노
부정 점수: 3/5 점
학습 데이터에 감정만 존재하면 머신러닝 알고리즘 학습이 가능
05
감정 분석 + 머신러닝
기타 감정 분석 방법
문장 감정
영상미가 뛰어나고 너무너무 재미있었어요… 기쁨
허무한 결말… 분노
인생 최고의 영화… 감동
임베딩 벡터를 사용하여, 머신러닝 알고리즘 적용이 가능
05
감정 분석 + 머신러닝
기타 감정 분석 방법
엘리스
자연어
처리
.
.
.
[0.23, 0.401, 0.482, 0.42, 0.99 … ]
[0.03, 0.101, 2.682, 0.62, 0.08 … ]
[1.03, 0.222, 0.382, 0.32, 1.28 … ]
.
.
.
가장 간단한 방법으로 단어 임베딩 벡터의 평균을 사용
05
예시: 평균 임베딩 벡터
기타 감정 분석 방법
[텍스트 1] : 영상미가 | 뛰어나고 | 너무너무 | 재미있었어요 | …
영상미 : [0.12, 0.24, 0.913 …]
+
뛰어나다 : [0.87, 0.53, 0.03 …]
+
단어 임베딩 벡터에 필터를 적용하여 CNN 기반으로 감정 분류
05
예시: CNN
기타 감정 분석 방법
영상미 : [0.12, 0.24, 0.913 …]
뛰어나다 : [0.87, 0.53, 0.03 …]
너무너무 : [0.41, 0.23, 0.09 …]
재미있다 : [0.71, 0.51, 0.23 …]
필터 기쁨
문자 임베딩 벡터에 필터를 적용하여 CNN 기반으로 감정 분류
05
예시: CNN
기타 감정 분석 방법
영 : [0.52, 0.24, 0.33 …]
상 : [0.57, 0.13, 0.12 …]
미 : [0.17, 0.83, 0.63 …]
뛰 : [0.07, 0.93, 0.33 …]
필터 기쁨
LSTM, GRU를 활용하여 RNN 기반으로 분류 및 예측
05
예시: RNN
기타 감정 분석 방법
영상미가 뛰어나고 너무너무
…
추천
기쁨
문자 단위로 단어를 분리하여 RNN 기반으로 분류 및 예측
