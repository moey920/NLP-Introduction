# 문서 유사도 측정

- 문서는 다양한 요소와 이들의 상호작용으로 구성
    - 단어 - 형태소 - 문장 - 문단
    - 가장 기본 단위인 단어 조차 문서와 관련된 다양한 정보를 포함
        - 형태소, 키워드, 개체명(Named entity : 사람, 조직 등), 중의적 단어 등
    - 상위 개념인 문장 또한 추가적인 정보를 제공
        - 목적어, 주어, 문장 간 관계, 상호참조해결(여러 문장 사이에서 발생하는 객체가 무엇인지) 등 
    - 그렇기 때문에 문서의 가장 기본 단위인 **단어를 활용하여 문서를 표현**
    - 문서 유사도를 측정하기 위해 단어 기준으로 생성한 문서 벡터 간의 코사인 유사도를 사용
        - 정확한 문서 유사도 측정을 위해 문서의 특징을 잘 보존하는 벡터 표현 방식이 중요

# Bag of words

> 문서 내 **단어의 빈도수를 기준**으로 문서 **벡터를 생성**

- 직관적이고, 널리 사용된다.
- 자주 발생하는 단어가 문서의 특징을 나타낸다는 것을 가정
- Bag of words 문서 벡터의 차원은 데이터 내 발생하는 모든 단어의 개수와 동일
- Bag of words 문서 벡터는 합성어를 독립적인 단어로 개별 처리
    - log off / log is => log / off / is : 로그오프를 구분하지 못하고 통나무로 해석된다.

```
[문서 1] : these | are | five | IT | companies …
[문서 2] : these | five | great | singers …
```

||these |are |five |**IT** |companies |great |**singers** |a|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|문서1 |3 |7 |1 |**6** |1 |0 |0 |0|
|문서2 |2 |4 |1 |0 |0 |1 |**4** |0|

# Bag of N-grams

> N-gram은 연속된 N개의 단어를 기준으로 텍스트 분석을 수행

- Bag of N-grams은 **n-gram의 발생 빈도를 기준**으로 문서 벡터를 표현
    - 여러 n-gram도 사용할 수 있다.
- 자주 발생하는 단어가 문서의 주요 내용 및 특징을 항상 효과적으로 표현하지는 않음
    - 그리고, 그러나, 잘, 오늘, 만약 등

```
N = 1 (unigram) : 포근한 | 봄 | 날씨가 | 이어질 | 것으로 | 전망되며 …
N = 2 (bi-gram) : 포근한 봄 | 봄 날씨가 | 날씨가 이어질 | 이어질 것으로 | …
N = 3 (tri-gram) : 포근한 봄 날씨가 | 봄 날씨가 이어질 | 날씨가 이어질 것으로 | …
```

## TF-IDF((term frequency– inverse document frequency)

- 문서 내 상대적으로 자주 발생하는 단어가 더 중요하다는 점을 반영
- 단어 자체가 모든 데이터에서 많이 발생했다면 점수를 낮추어 문서 간 유사도를 반영할 때 큰 영향을 주지 못하도록 방지한다.
    - 단어의 상대적인 중요성(특정 문서에서의 중요성)을 높힐 수 있다.

```
문서 1에서 단어 "봄"의 TF-IDF 점수 = 
문서 1 내 "봄"의 빈도수 / 문서1 내 모든 단어의 빈도수 
× log(데이터 내 총 문서의 개수 / 데이터 내 "봄"이 들어간 문서의 개수)
```

doc2vec
03
벡터의 구성 요소가 직관적인 것은 bag of words 기반 기법의 큰 장점
03
Bag of words 기반 문서 벡터의 장점
doc2vec
그리고 그러나 잘 봄 컴퓨터 오늘 만약 …
문서 1 0 0 0 0.537 0 0 0 …
문서 2 0 0 0 0 0.783 0 0 …
텍스트 데이터의 양이 증가하면, 문서 벡터의 차원 증가
03
Bag of words 기반 문서 벡터의 단점
doc2vec
대부분 단어의 빈도수가 0인 희소(sparse) 벡터가 생성
03
Bag of words 기반 문서 벡터의 단점
doc2vec
그리고 그러나 잘 봄 컴퓨터 오늘 … 흐미
문서 1 0 0 0 0.537 0 0 … 0
문서 2 0 0 0 0 0.783 0 … 0
문서 벡터의 차원 증가에 따른 메모리 제약 및 비효율성 발생
03
Bag of words 기반 문서 벡터의 단점
doc2vec
문서 벡터의 차원 증가에 따른 차원의 저주 발생
03
Bag of words 기반 문서 벡터의 단점
doc2vec
doc2vec은 문서 내 단어 간 문맥적 유사도를 기반으로 문서 벡터를 임베딩
03
doc2vec
doc2vec
[문서 1] : 서울에 살고 있는 엘리스는 강아지를 좋아한다.
서울에
살고
엘리스는
강아지를
있는
문서 1
문서 내 단어의 임베딩 벡터를 학습하면서 문서의 임베딩 또한 지속적으로 학습
03
doc2vec
doc2vec
[문서 1] : 서울에 살고 있는 엘리스는 강아지를 좋아한다.
살고
있는
강아지를
좋아한다
엘리스는
문서 1
유사한 문맥의 문서 임베딩 벡터는 인접한 공간에 위치
03
doc2vec
doc2vec
날씨 기사 1
날씨 기사 2
스포츠 기사 1
스포츠 기사 2
경제 기사 2
경제 기사 1
doc2vec은 상대적으로 저차원의 공간에서 문서 벡터를 생성
03
doc2vec
doc2vec
살고
있는
강아지를
좋아한다
엘리스는
문서 1
Confidential all rights reserved
N-gram 기반 언어 모델
04
언어 모델이란 주어진 문장이 텍스트 데이터에서 발생할 확률을 계산하는 모델
04
언어 모델
N-gram 기반 언어 모델
문장 1 : 포근한 | 봄 | 날씨가 | 이어질 | 것으로 | 전망됩니다.
텍스트 데이터
P 문장 1 = 0.233
언어 모델을 통해 자동 문장 생성이 가능
04
언어 모델
N-gram 기반 언어 모델
챗봇 내 핵심 요소 중 하나
04
언어 모델
N-gram 기반 언어 모델
문장의 발생 확률은 단어가 발생할 조건부 확률의 곱으로 계산
04
언어 모델
N-gram 기반 언어 모델
문장 1 : 포근한 | 봄 | 날씨가 | 이어질 | 것으로 | 전망됩니다.
P 문장 1 = P 포근한 × P 봄 포근한) × P 날씨가 포근한, 봄) ×
P 이어질 포근한, 봄, 날씨가) × … × P 전망됩니다 포근한, 봄, 날씨가, 이어질, 것으로)
N-gram을 사용하여 단어의 조건부 확률을 근사
04
N-gram 기반 언어 모델
N-gram 기반 언어 모델
문장 1 : 포근한 | 봄 | 날씨가 | 이어질 | 것으로 | 전망됩니다.
Tri-gram 기준 P 문장 1 ≈ P 날씨가 포근한, 봄) × P 이어질 봄, 날씨가) × … ×
P 전망됩니다 이어질, 것으로)
각 N-gram 기반 조건부 확률은 데이터 내 각 n-gram의 빈도수로 계산
04
N-gram 기반 언어 모델
N-gram 기반 언어 모델
P 날씨가 포근한, 봄) =
전체 데이터 내 "포근한 봄 날씨가" 의 빈도수
전체 데이터에서 "포근한 봄"의 빈도수
문장 생성 시, 주어진 단어 기준 최대 조건부 확률의 단어를 다음 단어로 생성
04
N-gram 기반 언어 모델
N-gram 기반 언어 모델
생성되는 문장 : 무더운 | 여름 | ?
P 엘리스 | 여름 = 0.02
P 여름 | 바다 = 0.5
P 날씨 | 무더운, 여름 = 0.87
…
Confidential all rights reserved
RNN 기반 언어 모델
05
RNN으로 문장의 각 단어가 주어졌을 때 다음 단어를 예측하는 문제로 언어 모델 학습
05
RNN 기반 언어 모델
RNN 기반 언어 모델
문장 1 : 포근한 | 봄 | 날씨가 | 이어질 | 것으로 | 전망됩니다.
포근한
봄
봄
날씨가
날씨가
이어질
…
문자 단위 언어 모델로 학습 데이터 내 존재하지 않았던 단어 처리 및 생성 가능
05
RNN 기반 언어 모델
RNN 기반 언어 모델
문장 1 : 포근한 | 봄 | 날씨가 | 이어질 | 것으로 | 전망됩니다.
포
근
한
" "
날
씨
…
모델 학습 시, 문장의 시작과 종료를 의미하는 태그(tag) 추가
05
RNN 기반 언어 모델
RNN 기반 언어 모델
문장 1 : 포근한 | 봄 | 날씨가 | 이어질 | 것으로 | 전망됩니다.
학습 데이터 내 문장 1 : <Start> | 포근한 | 봄 | 날씨가 | … | 입니다 | <End>
문장 생성 시, 주어진 입력값부터 순차적으로 예측 단어 및 문자를 생성
05
RNN 기반 언어 모델
RNN 기반 언어 모델
입력값: 나
는
는
" "
가
고
…
고성능 언어 모델은 대용량 데이터와 이를 학습할 수 있는 하드웨어가 필수
