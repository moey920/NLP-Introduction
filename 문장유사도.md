# 문장 유사도

> 문장 간 유사도는 **공통된 단어** 혹은 **의미**를 기반으로 계산

```
[문장 1] : 오늘은 중부지방을 중심으로 소나기가 예상됩니다.
[문장 2] : 오늘은 전국이 맑은 날씨가 예상됩니다.
[문장 3] : 앞으로 접종 속도는 빨라질 것으로 예상됩니다.
```

## 자카드 지수

- 자카드(Jaccard) 지수는 문장 간 공통된 단어의 비율로 문장 간 유사도를 정의
    - 자카드 지수는 문장 간 유사도를 0 ~ 1 사이로 정의
    - 단어 기준으로만 정의하기 때문에 의미적인 내용을 판단할 수 없는 단점을 가지고 있다. 하지만 빠르고 쉬운 것이 장점이다.

`문장 1과 문장 2의 유사도 = (두 문장 내 공통된 단어의 종류) / (두 문장 내 모든 단어의 종류)`  

```
[문장 1] : '오늘은' 중부지방을 중심으로 소나기가 '예상됩니다'.
[문장 2] : '오늘은' 전국이 맑은 날씨가 '예상됩니다'.
문장 1과 문장 2의 유사도 = 2/8 = 0.25
```

### 자카드 지수를 통한 문장 유사도 측정

자카드 지수는 두 문장 간 공통된 단어의 비율로 문장 간 유사도를 측정합니다. 이번 실습에서는 직접 자카드 지수를 계산하는 cal_jaccard_sim함수를 구현하고, nltk에서 이미 구현된 자카드 거리 함수와 비교해 볼 예정입니다.

```
import nltk

sent_1 = "오늘 중부지방을 중심으로 소나기가 예상됩니다"
sent_2 = "오늘 전국이 맑은 날씨가 예상됩니다"

def cal_jaccard_sim(sent1, sent2):
    # 각 문장을 토큰화 후 set 타입으로 변환하세요.
    words_sent1 = set(sent1.split())
    words_sent2 = set(sent2.split())

    # 공통된 단어의 개수를 intersection 변수에 저장하세요.
    intersection = words_sent1.intersection(words_sent2)
    
    # 두 문장 내 발생하는 모든 단어의 개수를 union 변수에 저장하세요.
    union = words_sent1.union(words_sent2)

    # intersection과 union을 사용하여 자카드 지수를 계산하고 float 타입으로 반환하세요.
    return float(len(intersection) / len(union))

# cal_jaccard_sim() 함수 실행 결과를 확인합니다.
print(cal_jaccard_sim(sent_1, sent_2))

# nltk의 jaccard_distance() 함수를 이용해 자카드 유사도를 계산하세요.
# jaccard_distance(w1, w2)에 인자로 들어가는 w1, w2는 set 타입으로 정의된 각 문장별 단어 목록으로 만들어 주셔야 합니다.
# 유사도와 거리는 다음과 같은 관계로 정의가 됩니다: 유사도 = 1 - 거리
sent1_set = set(sent_1.split())
sent2_set = set(sent_2.split())
nltk_jaccard_sim = 1 - nltk.jaccard_distance(sent1_set, sent2_set)

# 직접 정의한 함수와 결과가 같은지 비교합니다.
print(nltk_jaccard_sim)
```

## 코사인 유사도

- **코사인 유사도**는 문장 벡터 간의 각도를 기반으로 계산
- 문장을 벡터로 변환한다.
    - 벡터 간의 각도는 벡터 간 내적을 사용해서 계산
    - 각도가 작을수록 비슷한 문장이다.
- 나이브 베이즈 기반에서 smilarty를 계산했을 때 사용한 단어 벡터간 유사도도 코사인 유사도 기반이다.
    ```
    A = [1, 3], B = [0 ,2]
    A와 B의 코사인 유사도 = A∙B / ||𝐴||||B||  = (1 × 0) + (3 × 2) / root(1^2+3^2) * root(0^2 + 2^2) = 6 / 2root(10) ≈ 0.9487
    ```
- 유클리드 거리와 같은 다양한 거리 지표가 존재
- 코사인 유사도는 **고차원의 공간에서 벡터 간의 유사성을 잘 보존하는 장점**이 있음
    - `d(p, q) = root(sum((qi-pi)^2))`
    - 문서를 모두 임베딩 벡터, 벡터화하기 때문에 길이가 매우 길다(매우 고차원에 존재한다.) -> 유클리드 거리보다는 각 벡터간의 각도를 사용하는 코사인 유사도가 훨씬 정보를 잘 보존한다.
    - 문장, 단어 유사도 모두 코사인 유사도를 사용하는 것이 권장된다.

### 코사인 유사도를 통한 문장 유사도 측정

코사인 유사도는 두 벡터 간의 각도를 사용하여 유사도를 측정합니다. 이번 실습에서는 직접 코사인 유사도를 계산하는 cal_cosine_sim함수를 구현하고, scipy와 scikit-learn에서 이미 구현된 코사인 거리 함수와 비교해 볼 예정입니다.

```
from numpy import sqrt, dot
from scipy.spatial import distance
from sklearn.metrics import pairwise

sent_1 = [0.3, 0.2, 0.2133, 0.3891, 0.8852, 0.586, 1.244, 0.777, 0.882]
sent_2 = [0.03, 0.223, 0.1, 0.4, 2.931, 0.122, 0.5934, 0.8472, 0.54]
sent_3 = [0.13, 0.83, 0.827, 0.92, 0.1, 0.32, 0.28, 0.34, 0]

def cal_cosine_sim(v1, v2):
    # 벡터 v1, v2 간 코사인 유사도를 계산 후 반환하세요.
    # 벡터 간 내적은 numpy의 dot()함수를, 루트는 numpy의 sqrt()함수를 사용합니다. 벡터 v1의 크기는 dot(v1, v1)의 루트값으로 계산할 수 있습니다.
    top = dot(v1, v2) # v1, v2의 내적
    size1 = sqrt(dot(v1, v1))
    size2 = sqrt(dot(v2, v2))
    
    return top / (size1 * size2)

# 정의한 코사인 유도 계산 함수를 확인합니다.
print(cal_cosine_sim(sent_1, sent_2))

# scipy의 distance.cosine() 함수를 이용한 코사인 유사도를 계산하세요.
# istance.cosine()은 코사인 거리를 계산하기 때문에 유사도 = 1 - 거리 관계를 사용해서 유사도로 변환
scipy_cosine_sim = 1 - distance.cosine(sent_1, sent_2)

# scipy를 이용해 계산한 코사인 유사도를 확인합니다.
print(scipy_cosine_sim)

# scikit-learn의 pairwise.cosine_similarity() 함수를 이용한 코사인 유사도를 계산하세요.
# pairwise.cosine_similarity() 함수의 인자로 변수 all_sent로 설정
all_sent = [sent_1] + [sent_2] + [sent_3]
scikit_learn_cosine_sim  = pairwise.cosine_similarity(all_sent) # 조합 가능한 모든 벡터의 쌍에 대한 코사인 유사도

# scikit-learn을 이용해 계산한 코사인 유사도를 확인합니다.
print(scikit_learn_cosine_sim)

'''
결과
0.713722489605211
0.7137224896052109
[[1.         0.71372249 0.4876509 ]
 [0.71372249 1.         0.2801926 ]
 [0.4876509  0.2801926  1.        ]]
'''
```
